# çˆ¬èŸ²é–‹ç™¼å¾…è¾¦æ¸…å–®å’Œæ¸¬è©¦æŒ‡å—

## ğŸ“ é–‹ç™¼å¾…è¾¦æ¸…å–®

### Phase 1: åŸºç¤æ¶æ§‹ âœ… (å·²æä¾›ç¯„ä¾‹ä»£ç¢¼)
- [x] æ“´å±• `CrawledContent` æ¨¡å‹
- [x] å»ºç«‹ `CrawlerService` åŸºç¤æ¶æ§‹
- [x] è¨­è¨ˆæ•´åˆä»‹é¢

### Phase 2: æ ¸å¿ƒåŠŸèƒ½é–‹ç™¼
- [ ] **å¯¦ç¾åŸºæœ¬çˆ¬èŸ²åŠŸèƒ½**
  - [ ] å–®ä¸€ URL å…§å®¹æå–
  - [ ] HTTP è«‹æ±‚è™•ç†å’ŒéŒ¯èª¤æ¢å¾©
  - [ ] HTML è§£æå’Œå…§å®¹æ¸…ç†
  - [ ] åŸºæœ¬çš„å…§å®¹å“è³ªæª¢æŸ¥

- [ ] **æ–°èç¶²ç«™é©é…å™¨**
  - [ ] ä¸­å¤®ç¤¾ (CNA) é©é…å™¨
  - [ ] è‡ªç”±æ™‚å ± (LTN) é©é…å™¨  
  - [ ] è¯åˆå ± (UDN) é©é…å™¨
  - [ ] é€šç”¨ RSS è§£æå™¨

- [ ] **å…§å®¹è™•ç†åŠŸèƒ½**
  - [ ] ä¸­æ–‡åˆ†è©å’Œé—œéµè©æå–
  - [ ] å…§å®¹æ‘˜è¦è‡ªå‹•ç”Ÿæˆ
  - [ ] é‡è¤‡å…§å®¹æª¢æ¸¬
  - [ ] å…§å®¹å“è³ªè©•åˆ†ç³»çµ±

### Phase 3: é«˜ç´šåŠŸèƒ½
- [ ] **æœå°‹åŠŸèƒ½**
  - [ ] é—œéµè©æœå°‹å¯¦ç¾
  - [ ] æ—¥æœŸç¯„åœéæ¿¾
  - [ ] å¤šä¾†æºä¸¦è¡Œçˆ¬å–
  - [ ] çµæœæ’åºå’Œç¯©é¸

- [ ] **æ•ˆèƒ½å„ªåŒ–**
  - [ ] éåŒæ­¥ä¸¦ç™¼è™•ç†
  - [ ] è«‹æ±‚é€Ÿç‡é™åˆ¶
  - [ ] å¿«å–æ©Ÿåˆ¶
  - [ ] è³‡æ–™åº«å„²å­˜ (å¯é¸)

### Phase 4: API å’Œæ•´åˆ
- [ ] **REST API ç«¯é»**
  - [ ] `/crawl/keywords` - é—œéµè©æœå°‹
  - [ ] `/crawl/urls` - URL åˆ—è¡¨çˆ¬å–
  - [ ] `/crawl/sources` - æ”¯æ´çš„ä¾†æºåˆ—è¡¨
  - [ ] `/generate-from-news` - æ–°èè½‰æ’­å®¢

- [ ] **æ•´åˆæ¸¬è©¦**
  - [ ] çˆ¬èŸ²â†’AI æ’­å®¢ç”Ÿæˆç®¡ç·šæ¸¬è©¦
  - [ ] éŒ¯èª¤è™•ç†æ¸¬è©¦
  - [ ] æ•ˆèƒ½è² è¼‰æ¸¬è©¦

## ğŸ§ª æ¸¬è©¦æŒ‡å—

### 1. å–®å…ƒæ¸¬è©¦ç¯„ä¾‹

```python
# tests/test_crawler_service.py
import pytest
from app.services.crawler_service import CrawlerService
from app.models.crawler import CrawlRequest

@pytest.mark.asyncio
async def test_crawl_single_url():
    """æ¸¬è©¦å–®ä¸€ URL çˆ¬å–"""
    crawler = CrawlerService()
    
    # æ¸¬è©¦ä¸€å€‹å·²çŸ¥çš„æ–°è URL
    test_url = "https://www.cna.com.tw/news/aipl/202307220001.aspx"
    result = await crawler._crawl_single_url(test_url)
    
    assert result is not None
    assert len(result.title) > 0
    assert len(result.content) > 200
    assert result.url == test_url
    assert result.source == "cna"

@pytest.mark.asyncio 
async def test_keyword_search():
    """æ¸¬è©¦é—œéµè©æœå°‹"""
    crawler = CrawlerService()
    
    request = CrawlRequest(
        keywords=["å°ç£", "ç§‘æŠ€"],
        max_results=3,
        min_word_count=200
    )
    
    results = await crawler.crawl_by_keywords(request)
    
    assert len(results) <= 3
    for result in results:
        assert result.word_count >= 200
        # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµè©
        content_lower = result.content.lower()
        assert "å°ç£" in content_lower or "ç§‘æŠ€" in content_lower

def test_content_deduplication():
    """æ¸¬è©¦å…§å®¹å»é‡"""
    crawler = CrawlerService()
    
    # å‰µå»ºå…©ç¯‡å…§å®¹ç›¸åŒçš„æ–‡ç« 
    from app.models.crawler import CrawledContent
    from datetime import datetime
    
    content1 = CrawledContent(
        title="æ¸¬è©¦æ¨™é¡Œ1",
        summary="æ¸¬è©¦æ‘˜è¦", 
        content="é€™æ˜¯æ¸¬è©¦å…§å®¹",
        url="http://test1.com",
        source="test",
        crawl_timestamp=datetime.now(),
        content_hash=crawler._calculate_hash("é€™æ˜¯æ¸¬è©¦å…§å®¹"),
        word_count=7
    )
    
    content2 = CrawledContent(
        title="æ¸¬è©¦æ¨™é¡Œ2",
        summary="æ¸¬è©¦æ‘˜è¦",
        content="é€™æ˜¯æ¸¬è©¦å…§å®¹",  # ç›¸åŒå…§å®¹
        url="http://test2.com", 
        source="test",
        crawl_timestamp=datetime.now(),
        content_hash=crawler._calculate_hash("é€™æ˜¯æ¸¬è©¦å…§å®¹"),
        word_count=7
    )
    
    request = CrawlRequest(max_results=10, min_word_count=1)
    filtered = crawler._filter_and_process([content1, content2], request)
    
    # å»é‡å¾Œæ‡‰è©²åªæœ‰ä¸€ç¯‡
    assert len(filtered) == 1

def test_keyword_extraction():
    """æ¸¬è©¦é—œéµè©æå–"""
    crawler = CrawlerService()
    
    text = "å°ç£ç§‘æŠ€ç”¢æ¥­ç™¼å±•è¿…é€Ÿï¼Œäººå·¥æ™ºæ…§å’ŒåŠå°é«”æŠ€è¡“é ˜å…ˆå…¨çƒã€‚"
    keywords = crawler._extract_keywords(text)
    
    assert "å°ç£" in keywords
    assert "ç§‘æŠ€" in keywords
    assert "äººå·¥æ™ºæ…§" in keywords or "æ™ºæ…§" in keywords
    assert len(keywords) > 0
```

### 2. æ•´åˆæ¸¬è©¦ç¯„ä¾‹

```python
# tests/test_integration.py
import pytest
from app.services.crawler_service import CrawlerService
from app.services.pydantic_ai_service import PydanticAIService
from app.models.podcast import PodcastGenerationRequest
from app.models.crawler import CrawlRequest

@pytest.mark.asyncio
async def test_news_to_podcast_pipeline():
    """æ¸¬è©¦å¾æ–°èçˆ¬å–åˆ°æ’­å®¢ç”Ÿæˆçš„å®Œæ•´ç®¡ç·š"""
    
    # Step 1: çˆ¬å–æ–°è
    crawler = CrawlerService()
    crawl_request = CrawlRequest(
        keywords=["äººå·¥æ™ºæ…§"],
        max_results=2,
        min_word_count=300
    )
    
    crawled_content = await crawler.crawl_by_keywords(crawl_request)
    assert len(crawled_content) > 0
    
    # Step 2: ç”Ÿæˆæ’­å®¢
    ai_service = PydanticAIService(use_twcc=True)
    
    # çµ„åˆæ–°èæ¨™é¡Œä½œç‚ºä¸»é¡Œ
    combined_topic = f"AI æ–°èåˆ†æ: {crawled_content[0].title}"
    
    podcast_request = PodcastGenerationRequest(
        topic=combined_topic,
        tone="educational", 
        duration=15
    )
    
    result = await ai_service.generate_complete_podcast_content(
        podcast_request,
        crawled_content=crawled_content
    )
    
    # é©—è­‰çµæœ
    assert result["success"] is True
    assert "structured_script" in result
    script = result["structured_script"]
    assert len(script["title"]) > 0
    assert len(script["full_dialogue"]) > 500  # ç¢ºä¿æœ‰è¶³å¤ çš„å°è©±å…§å®¹
    assert "ä¸»æŒäººA" in script["full_dialogue"]
    assert "ä¸»æŒäººB" in script["full_dialogue"]

@pytest.mark.asyncio
async def test_multiple_sources():
    """æ¸¬è©¦å¤šä¾†æºçˆ¬å–"""
    crawler = CrawlerService()
    
    # æ¸¬è©¦ä¸åŒä¾†æº
    urls = [
        "https://www.cna.com.tw/news/ait/202307220001.aspx",
        "https://news.ltn.com.tw/news/focus/breakingnews/4365001",
        # æ·»åŠ æ›´å¤šæ¸¬è©¦ URL
    ]
    
    results = await crawler.crawl_from_urls(urls)
    
    # æ‡‰è©²è‡³å°‘æœ‰ä¸€å€‹æˆåŠŸçš„çµæœ
    assert len(results) > 0
    
    # æª¢æŸ¥ä¾†æºè­˜åˆ¥æ˜¯å¦æ­£ç¢º
    sources = {result.source for result in results}
    assert len(sources) > 0  # è‡³å°‘è­˜åˆ¥å‡ºä¸€å€‹ä¾†æº
```

### 3. æ•ˆèƒ½æ¸¬è©¦ç¯„ä¾‹

```python
# tests/test_performance.py
import pytest
import asyncio
import time
from app.services.crawler_service import CrawlerService

@pytest.mark.asyncio
async def test_concurrent_crawling():
    """æ¸¬è©¦ä¸¦ç™¼çˆ¬å–æ•ˆèƒ½"""
    crawler = CrawlerService()
    
    # æº–å‚™å¤šå€‹ URL
    urls = [
        "https://www.cna.com.tw/news/aipl/202307220001.aspx",
        "https://www.cna.com.tw/news/ait/202307220002.aspx", 
        "https://www.cna.com.tw/news/asoc/202307220003.aspx",
        # æ›´å¤š URL...
    ]
    
    start_time = time.time()
    results = await crawler.crawl_from_urls(urls)
    end_time = time.time()
    
    # æª¢æŸ¥ä¸¦ç™¼æ•ˆèƒ½
    duration = end_time - start_time
    assert duration < 30  # æ‡‰è©²åœ¨ 30 ç§’å…§å®Œæˆ
    assert len(results) > 0

@pytest.mark.asyncio 
async def test_rate_limiting():
    """æ¸¬è©¦é€Ÿç‡é™åˆ¶"""
    crawler = CrawlerService()
    
    # é€£çºŒè«‹æ±‚åŒä¸€ä¾†æº
    base_url = "https://www.cna.com.tw"
    
    start_time = time.time()
    
    # æ¨¡æ“¬ 5 å€‹é€£çºŒè«‹æ±‚
    tasks = []
    for i in range(5):
        url = f"{base_url}/news/aipl/20230722000{i}.aspx"
        tasks.append(crawler._crawl_single_url(url))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()
    
    duration = end_time - start_time
    # ç¢ºä¿æœ‰é©ç•¶çš„é€Ÿç‡é™åˆ¶ (ä¸æœƒå¤ªå¿«)
    assert duration > 2  # è‡³å°‘éœ€è¦ 2 ç§’ (æ¯ç§’æœ€å¤š 2 å€‹è«‹æ±‚)
```

## ğŸš€ å¿«é€Ÿé–‹å§‹æŒ‡å—

### 1. ç’°å¢ƒè¨­å®š
```bash
# å®‰è£é¡å¤–çš„çˆ¬èŸ²ä¾è³´
pip install beautifulsoup4 lxml httpx jieba newspaper3k

# æˆ–æ·»åŠ åˆ° requirements.txt
echo "beautifulsoup4==4.12.2" >> requirements.txt
echo "lxml==4.9.3" >> requirements.txt  
echo "httpx==0.24.1" >> requirements.txt
echo "jieba==0.42.1" >> requirements.txt
```

### 2. é–‹ç™¼æ­¥é©Ÿ
1. å…ˆå¯¦ç¾ `_crawl_single_url` æ–¹æ³•ï¼Œæ¸¬è©¦åŸºæœ¬çˆ¬å–åŠŸèƒ½
2. æ·»åŠ ç¬¬ä¸€å€‹æ–°èç¶²ç«™é©é…å™¨ (å»ºè­°å¾ä¸­å¤®ç¤¾é–‹å§‹)
3. å¯¦ç¾é—œéµè©æœå°‹åŠŸèƒ½
4. æ·»åŠ å…§å®¹è™•ç†å’Œå“è³ªè©•åˆ†
5. æ•´åˆåˆ°æ’­å®¢ç”Ÿæˆç³»çµ±
6. æ·»åŠ æ›´å¤šæ–°èä¾†æº

### 3. æ¸¬è©¦åŸ·è¡Œ
```bash
# åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
pytest tests/ -v

# åŸ·è¡Œç‰¹å®šæ¸¬è©¦
pytest tests/test_crawler_service.py::test_crawl_single_url -v

# åŸ·è¡Œæ•´åˆæ¸¬è©¦
pytest tests/test_integration.py -v
```

é€™å€‹æŒ‡å—ç‚ºæ‚¨çš„æœ‹å‹æä¾›äº†å®Œæ•´çš„é–‹ç™¼è·¯ç·šåœ–å’Œæ¸¬è©¦æ¡†æ¶ï¼Œè®“ä»–èƒ½å¤ ç³»çµ±æ€§åœ°é–‹ç™¼å’Œæ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½ã€‚
