"""
æ•´åˆæ¸¬è©¦ï¼šçˆ¬èŸ² + AI æ’­å®¢ç”Ÿæˆå®Œæ•´æµç¨‹
ä½¿ç”¨ç¾æœ‰çš„ crawl4ai_service æŠ“å–æ–°èï¼Œç„¶å¾Œç”Ÿæˆæ’­å®¢
"""
import asyncio
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.services.crawl4ai_service import crawl_news
from app.services.pydantic_ai_service import PydanticAIService
from app.models.podcast import PodcastGenerationRequest

async def test_full_pipeline():
    """æ¸¬è©¦å®Œæ•´çš„çˆ¬èŸ²â†’AIæ’­å®¢ç”Ÿæˆç®¡ç·š"""
    
    print("ğŸš€ é–‹å§‹å®Œæ•´ç®¡ç·šæ¸¬è©¦")
    print("=" * 60)
    
    # Step 1: ä½¿ç”¨çˆ¬èŸ²ç²å–æ–°è
    print("ğŸ“° Step 1: çˆ¬å–æ–°èå…§å®¹...")
    try:
        # æ¸¬è©¦æŠ“å–ç§‘æŠ€æ–°è
        crawled_articles = await crawl_news("technology_news", max_articles=3)
        
        if not crawled_articles:
            print("âŒ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ–°èï¼Œä½¿ç”¨å‚™ç”¨å…§å®¹")
            # å¦‚æœçˆ¬èŸ²å¤±æ•—ï¼Œä½¿ç”¨æ¨¡æ“¬å…§å®¹
            from app.models.crawler import CrawledContent, ContentType
            from datetime import datetime
            
            crawled_articles = [
                CrawledContent(
                    id="test-1",
                    title="AI æŠ€è¡“æœ€æ–°çªç ´ï¼šå¤§å‹èªè¨€æ¨¡å‹åœ¨å¤šæ¨¡æ…‹æ‡‰ç”¨",
                    content="æœ€æ–°ç ”ç©¶é¡¯ç¤ºï¼Œå¤§å‹èªè¨€æ¨¡å‹åœ¨çµåˆè¦–è¦ºã€éŸ³è¨Šç­‰å¤šæ¨¡æ…‹è³‡æ–™æ–¹é¢å–å¾—é‡å¤§çªç ´...",
                    summary="AIæŠ€è¡“åœ¨å¤šæ¨¡æ…‹æ‡‰ç”¨æ–¹é¢çš„æœ€æ–°é€²å±•",
                    url="https://example.com/ai-breakthrough",
                    source="tech_news",
                    published_at=datetime.now(),
                    crawled_at=datetime.now(),
                    content_type=ContentType.NEWS,
                    topic="technology_news",
                    keywords=["AI", "èªè¨€æ¨¡å‹", "å¤šæ¨¡æ…‹"],
                    relevance_score=0.9
                )
            ]
        
        print(f"âœ… æˆåŠŸç²å– {len(crawled_articles)} ç¯‡æ–°è")
        for i, article in enumerate(crawled_articles, 1):
            print(f"   {i}. {article.title}")
            print(f"      ä¾†æº: {article.source}")
            print(f"      æ‘˜è¦: {article.summary[:100]}...")
            print()
    
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²éšæ®µå¤±æ•—: {e}")
        return
    
    # Step 2: åˆ†ææ–°èå…§å®¹ä¸¦æº–å‚™æ’­å®¢ä¸»é¡Œ
    print("ğŸ¤– Step 2: åˆ†ææ–°èä¸¦æº–å‚™ AI ç”Ÿæˆ...")
    
    # çµ„åˆæ–°èå…§å®¹ç‚ºæ’­å®¢ä¸»é¡Œ
    main_titles = [article.title for article in crawled_articles[:2]]
    combined_content = "\n\n".join([
        f"æ–°è {i+1}: {article.title}\nå…§å®¹: {article.content[:300]}..."
        for i, article in enumerate(crawled_articles[:3])
    ])
    
    podcast_topic = f"""
    åŸºæ–¼ä»¥ä¸‹æœ€æ–°ç§‘æŠ€æ–°èï¼Œè«‹ç”Ÿæˆä¸€å€‹å°ˆæ¥­çš„é›™ä¸»æŒäººå°è©±å¼æ’­å®¢è…³æœ¬ï¼š

    ä¸»è¦æ–°èæ¨™é¡Œï¼š
    {' | '.join(main_titles)}
    
    è©³ç´°æ–°èå…§å®¹ï¼š
    {combined_content}
    
    è«‹ç”Ÿæˆä¸€å€‹15-18åˆ†é˜çš„ç§‘æŠ€åˆ†ææ’­å®¢ï¼Œè¦æ±‚ï¼š
    1. æ·±åº¦åˆ†æé€™äº›ç§‘æŠ€ç™¼å±•çš„æ„ç¾©å’Œå½±éŸ¿
    2. è¨è«–å°ç”¢æ¥­å’Œç¤¾æœƒçš„æ½›åœ¨è®ŠåŒ–
    3. é›™ä¸»æŒäººè‡ªç„¶å°è©±é¢¨æ ¼
    4. å°ˆæ¥­ä½†æ˜“æ‡‚çš„è§£èªªæ–¹å¼
    5. åŒ…å«æœªä¾†è¶¨å‹¢é æ¸¬
    """
    
    print("ğŸ“ æº–å‚™çš„æ’­å®¢ä¸»é¡Œ:")
    print(f"   ä¸»è¦æ¨™é¡Œ: {main_titles[0] if main_titles else 'ç„¡'}")
    print(f"   æ–°èæ•¸é‡: {len(crawled_articles)}")
    print()
    
    # Step 3: ä½¿ç”¨ AI ç”Ÿæˆæ’­å®¢
    print("ğŸ™ï¸ Step 3: AI ç”Ÿæˆæ’­å®¢è…³æœ¬...")
    
    try:
        # åˆå§‹åŒ– AI æœå‹™
        ai_service = PydanticAIService(use_twcc=True)
        
        # å‰µå»ºæ’­å®¢è«‹æ±‚
        request = PodcastGenerationRequest(
            topic=podcast_topic,
            tone="educational",
            duration=18
        )
        
        # ç”Ÿæˆæ’­å®¢ï¼ˆå°‡çˆ¬èŸ²å…§å®¹å‚³çµ¦ AIï¼‰
        result = await ai_service.generate_complete_podcast_content(
            request, 
            crawled_content=crawled_articles  # é€™è£¡å‚³å…¥çˆ¬èŸ²ç²å–çš„å…§å®¹
        )
        
        if result.get("success"):
            script = result['structured_script']
            
            print("ğŸ‰ å®Œæ•´ç®¡ç·šæ¸¬è©¦æˆåŠŸï¼")
            print("=" * 60)
            print(f"ğŸ™ï¸ {script['title']}")
            print(f"ğŸ§ ä¸»æŒäºº: {' èˆ‡ '.join(script['hosts'])}")
            print()
            
            # é¡¯ç¤ºç”Ÿæˆçš„å°è©±ï¼ˆå‰500å­—ï¼‰
            dialogue = script['full_dialogue']
            print("ğŸ“œ ç”Ÿæˆçš„æ’­å®¢å°è©±ï¼ˆé è¦½ï¼‰:")
            print(dialogue[:800] + "..." if len(dialogue) > 800 else dialogue)
            
            print("\n" + "=" * 60)
            print("ğŸ“Š æ’­å®¢è³‡è¨Š:")
            print(f"â±ï¸  é ä¼°æ™‚é•·: {script['estimated_duration_minutes']} åˆ†é˜")
            print(f"ğŸ¯ é—œéµè­°é¡Œ: {', '.join(script['key_points'])}")
            print(f"ğŸ“š è³‡æ–™ä¾†æº: {', '.join(script['sources_mentioned'])}")
            
            # é¡¯ç¤ºä½¿ç”¨çš„æ–°èä¾†æº
            print(f"\nğŸ“° ä½¿ç”¨çš„æ–°èä¾†æº:")
            for article in crawled_articles:
                print(f"   â€¢ {article.title} ({article.source})")
            
            model_info = result['model_info']
            print(f"\nğŸ¤– æŠ€è¡“è³‡è¨Š:")
            print(f"   AI æä¾›å•†: {model_info['provider']}")
            print(f"   æ¨¡å‹: {model_info['model_name']}")
            
        else:
            print(f"âŒ AI ç”Ÿæˆå¤±æ•—: {result.get('error')}")
    
    except Exception as e:
        print(f"âŒ AI ç”Ÿæˆéšæ®µå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ™ï¸ Hakkast çˆ¬èŸ²+AI æ’­å®¢å®Œæ•´ç®¡ç·šæ¸¬è©¦")
    print("ğŸ”— æ¸¬è©¦æµç¨‹: æ–°èçˆ¬å– â†’ å…§å®¹åˆ†æ â†’ AIæ’­å®¢ç”Ÿæˆ")
    print()
    asyncio.run(test_full_pipeline())
